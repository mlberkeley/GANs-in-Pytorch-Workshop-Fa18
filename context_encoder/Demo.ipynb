{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "from torch.autograd import Variable\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the weights from this link\n",
    "# https://drive.google.com/file/d/0B6oeoQaX0xmzS0RXXzNYZkZ3ZUk/view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Options:\n",
    "    def __init__(self, test_image, netG):\n",
    "        self.dataset = 'streetview'\n",
    "        self.dataroot ='dataset/train' # path to dataset\n",
    "        self.test_image = test_image\n",
    "        self.netG = netG\n",
    "        self.netD = ''\n",
    "        self.workers = 4 # number of workers helping load data\n",
    "        self.batchSize = 64\n",
    "        self.imageSize = 128 # the height / width of the input image to network\n",
    "        self.nz = 100 # size of the latent z vector\n",
    "        self.ngf = 64\n",
    "        self.ndf = 64\n",
    "        self.nc = 3 # number of channels, 3 for rgb colored images\n",
    "        self.niter = 200 # number of epochs to train for\n",
    "        self.lr = 0.0002 # learning rate\n",
    "        self.beta1 = 0.5 # hyperparameter for adam optimizer\n",
    "        self.cuda = False\n",
    "        self.ngpu = 1\n",
    "        self.outf = '.' # folder to output images and model checkpoints\n",
    "        self.manualSeed = None\n",
    "        self.nBottleneck = 4000 # of dim for bottleneck of encoder\n",
    "        self.overlapPred = 4 # overlapping edges\n",
    "        self.nef = 64 # of encoder filters in first conv layer\n",
    "        self.wtl2 = 0.999 # 0 means do not use else use with this weight\n",
    "        self.wtlD = 0.001 # 0 means do not use else use with this weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GAN model used\n",
    "\n",
    "# Generator\n",
    "# Input is the 128x128 image with center missing\n",
    "# Output is the 64x64 center image\n",
    "class _netG(nn.Module):\n",
    "    def __init__(self, opt):\n",
    "        super(_netG, self).__init__()\n",
    "        self.ngpu = opt.ngpu\n",
    "        self.main = nn.Sequential(\n",
    "            # input is (nc) x 128 x 128\n",
    "            nn.Conv2d(opt.nc,opt.nef,4,2,1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size: (nef) x 64 x 64\n",
    "            nn.Conv2d(opt.nef,opt.nef,4,2,1, bias=False),\n",
    "            nn.BatchNorm2d(opt.nef),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size: (nef) x 32 x 32\n",
    "            nn.Conv2d(opt.nef,opt.nef*2,4,2,1, bias=False),\n",
    "            nn.BatchNorm2d(opt.nef*2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size: (nef*2) x 16 x 16\n",
    "            nn.Conv2d(opt.nef*2,opt.nef*4,4,2,1, bias=False),\n",
    "            nn.BatchNorm2d(opt.nef*4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size: (nef*4) x 8 x 8\n",
    "            nn.Conv2d(opt.nef*4,opt.nef*8,4,2,1, bias=False),\n",
    "            nn.BatchNorm2d(opt.nef*8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size: (nef*8) x 4 x 4\n",
    "            nn.Conv2d(opt.nef*8,opt.nBottleneck,4, bias=False),\n",
    "            # tate size: (nBottleneck) x 1 x 1\n",
    "            nn.BatchNorm2d(opt.nBottleneck),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # input is Bottleneck, going into a convolution\n",
    "            nn.ConvTranspose2d(opt.nBottleneck, opt.ngf * 8, 4, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(opt.ngf * 8),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*8) x 4 x 4\n",
    "            nn.ConvTranspose2d(opt.ngf * 8, opt.ngf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(opt.ngf * 4),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*4) x 8 x 8\n",
    "            nn.ConvTranspose2d(opt.ngf * 4, opt.ngf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(opt.ngf * 2),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*2) x 16 x 16\n",
    "            nn.ConvTranspose2d(opt.ngf * 2, opt.ngf, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(opt.ngf),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf) x 32 x 32\n",
    "            nn.ConvTranspose2d(opt.ngf, opt.nc, 4, 2, 1, bias=False),\n",
    "            nn.Tanh()\n",
    "            # state size. (nc) x 64 x 64\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        if isinstance(input.data, torch.cuda.FloatTensor) and self.ngpu > 1:\n",
    "            output = nn.parallel.data_parallel(self.main, input, range(self.ngpu))\n",
    "        else:\n",
    "            output = self.main(input)\n",
    "        return output\n",
    "\n",
    "# Discriminator\n",
    "class _netlocalD(nn.Module):\n",
    "    def __init__(self, opt):\n",
    "        super(_netlocalD, self).__init__()\n",
    "        self.ngpu = opt.ngpu\n",
    "        self.main = nn.Sequential(\n",
    "            # input is (nc) x 64 x 64\n",
    "            nn.Conv2d(opt.nc, opt.ndf, 4, 2, 1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf) x 32 x 32\n",
    "            nn.Conv2d(opt.ndf, opt.ndf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(opt.ndf * 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf*2) x 16 x 16\n",
    "            nn.Conv2d(opt.ndf * 2, opt.ndf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(opt.ndf * 4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf*4) x 8 x 8\n",
    "            nn.Conv2d(opt.ndf * 4, opt.ndf * 8, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(opt.ndf * 8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf*8) x 4 x 4\n",
    "            nn.Conv2d(opt.ndf * 8, 1, 4, 1, 0, bias=False),\n",
    "            # This activationensures the final value will be between 0 and 1 \n",
    "            # and can be used as a probability measure\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        if isinstance(input.data, torch.cuda.FloatTensor) and self.ngpu > 1:\n",
    "            output = nn.parallel.data_parallel(self.main, input, range(self.ngpu))\n",
    "        else:\n",
    "            output = self.main(input)\n",
    "\n",
    "        return output.view(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-5-6b1be8a2d21e>, line 8)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-5-6b1be8a2d21e>\"\u001b[0;36m, line \u001b[0;32m8\u001b[0m\n\u001b[0;31m    this is not code it is words so will break # Comment this out to actually train\u001b[0m\n\u001b[0m                      ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Training models from scratch, don't worry about this cell, \n",
    "# all the interesting things about the training are happening in the next one\n",
    "\n",
    "# We will not actually be training this model but rather using pretrained\n",
    "# weights for the sake of time. Feel free to look at the code in train.py\n",
    "# on your own though if you are interested in training your own model\n",
    "\n",
    "this is not code it is words so will break # Comment this out to actually train\n",
    "\n",
    "try:\n",
    "    os.makedirs(\"result/train/cropped\")\n",
    "    os.makedirs(\"result/train/real\")\n",
    "    os.makedirs(\"result/train/recon\")\n",
    "    os.makedirs(\"model\")\n",
    "except OSError:\n",
    "    pass\n",
    "\n",
    "if opt.manualSeed is None:\n",
    "    opt.manualSeed = random.randint(1, 10000)\n",
    "print(\"Random Seed: \", opt.manualSeed)\n",
    "random.seed(opt.manualSeed)\n",
    "torch.manual_seed(opt.manualSeed)\n",
    "if opt.cuda:\n",
    "    torch.cuda.manual_seed_all(opt.manualSeed)\n",
    "\n",
    "cudnn.benchmark = True\n",
    "\n",
    "if torch.cuda.is_available() and not opt.cuda:\n",
    "    print(\"WARNING: You have a CUDA device, so you should probably run with --cuda\")\n",
    "\n",
    "if opt.dataset in ['imagenet', 'folder', 'lfw']:\n",
    "    # folder dataset\n",
    "    dataset = dset.ImageFolder(root=opt.dataroot,\n",
    "                               transform=transforms.Compose([\n",
    "                                   transforms.Scale(opt.imageSize),\n",
    "                                   transforms.CenterCrop(opt.imageSize),\n",
    "                                   transforms.ToTensor(),\n",
    "                                   transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "                               ]))\n",
    "elif opt.dataset == 'lsun':\n",
    "    dataset = dset.LSUN(db_path=opt.dataroot, classes=['bedroom_train'],\n",
    "                        transform=transforms.Compose([\n",
    "                            transforms.Scale(opt.imageSize),\n",
    "                            transforms.CenterCrop(opt.imageSize),\n",
    "                            transforms.ToTensor(),\n",
    "                            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "                        ]))\n",
    "elif opt.dataset == 'cifar10':\n",
    "    dataset = dset.CIFAR10(root=opt.dataroot, download=True,\n",
    "                           transform=transforms.Compose([\n",
    "                               transforms.Scale(opt.imageSize),\n",
    "                               transforms.ToTensor(),\n",
    "                               transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "                           ])\n",
    "    )\n",
    "elif opt.dataset == 'streetview':\n",
    "    transform = transforms.Compose([transforms.Scale(opt.imageSize),\n",
    "                                    transforms.CenterCrop(opt.imageSize),\n",
    "                                    transforms.ToTensor(),\n",
    "                                    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "    dataset = dset.ImageFolder(root=opt.dataroot, transform=transform )\n",
    "assert dataset\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=opt.batchSize,\n",
    "                                         shuffle=True, num_workers=int(opt.workers))\n",
    "\n",
    "ngpu = int(opt.ngpu)\n",
    "nz = int(opt.nz)\n",
    "ngf = int(opt.ngf)\n",
    "ndf = int(opt.ndf)\n",
    "nc = 3\n",
    "nef = int(opt.nef)\n",
    "nBottleneck = int(opt.nBottleneck)\n",
    "wtl2 = float(opt.wtl2)\n",
    "overlapL2Weight = 10\n",
    "\n",
    "# custom weights initialization called on netG and netD\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        m.weight.data.normal_(0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        m.weight.data.normal_(1.0, 0.02)\n",
    "        m.bias.data.fill_(0)\n",
    "\n",
    "\n",
    "resume_epoch=0\n",
    "\n",
    "netG = _netG(opt)\n",
    "netG.apply(weights_init)\n",
    "if opt.netG != '':\n",
    "    netG.load_state_dict(torch.load(opt.netG,map_location=lambda storage, location: storage)['state_dict'])\n",
    "    resume_epoch = torch.load(opt.netG)['epoch']\n",
    "print(netG)\n",
    "\n",
    "\n",
    "netD = _netlocalD(opt)\n",
    "netD.apply(weights_init)\n",
    "if opt.netD != '':\n",
    "    netD.load_state_dict(torch.load(opt.netD,map_location=lambda storage, location: storage)['state_dict'])\n",
    "    resume_epoch = torch.load(opt.netD)['epoch']\n",
    "print(netD)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-6-8fe5461b0e7c>, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-6-8fe5461b0e7c>\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    this is not code it is words so will break # Comment this out to actually train\u001b[0m\n\u001b[0m                      ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# This is where the interesting parts of the training code happens\n",
    "\n",
    "this is not code it is words so will break # Comment this out to actually train\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "criterionMSE = nn.MSELoss()\n",
    "\n",
    "input_real = torch.FloatTensor(opt.batchSize, 3, opt.imageSize, opt.imageSize)\n",
    "input_cropped = torch.FloatTensor(opt.batchSize, 3, opt.imageSize, opt.imageSize)\n",
    "label = torch.FloatTensor(opt.batchSize)\n",
    "real_label = 1\n",
    "fake_label = 0\n",
    "\n",
    "real_center = torch.FloatTensor(opt.batchSize, 3, opt.imageSize/2, opt.imageSize/2)\n",
    "\n",
    "# Just for speeding up with GPU's\n",
    "if opt.cuda:\n",
    "    netD.cuda()\n",
    "    netG.cuda()\n",
    "    criterion.cuda()\n",
    "    criterionMSE.cuda()\n",
    "    input_real, input_cropped,label = input_real.cuda(),input_cropped.cuda(), label.cuda()\n",
    "    real_center = real_center.cuda()\n",
    "\n",
    "\n",
    "input_real = Variable(input_real)\n",
    "input_cropped = Variable(input_cropped)\n",
    "label = Variable(label)\n",
    "\n",
    "\n",
    "real_center = Variable(real_center)\n",
    "\n",
    "# setup optimizer\n",
    "optimizerD = optim.Adam(netD.parameters(), lr=opt.lr, betas=(opt.beta1, 0.999))\n",
    "optimizerG = optim.Adam(netG.parameters(), lr=opt.lr, betas=(opt.beta1, 0.999))\n",
    "\n",
    "for epoch in range(resume_epoch,opt.niter):\n",
    "    for i, data in enumerate(dataloader, 0):\n",
    "        real_cpu, _ = data\n",
    "        # The true center of the image\n",
    "        real_center_cpu = real_cpu[:,:,int(opt.imageSize/4):int(opt.imageSize/4)+int(opt.imageSize/2),int(opt.imageSize/4):int(opt.imageSize/4)+int(opt.imageSize/2)]\n",
    "        batch_size = real_cpu.size(0)\n",
    "        input_real.data.resize_(real_cpu.size()).copy_(real_cpu)\n",
    "        input_cropped.data.resize_(real_cpu.size()).copy_(real_cpu)\n",
    "        real_center.data.resize_(real_center_cpu.size()).copy_(real_center_cpu)\n",
    "        # Cropped image with center missing\n",
    "        input_cropped.data[:,0,int(opt.imageSize/4+opt.overlapPred):int(opt.imageSize/4+opt.imageSize/2-opt.overlapPred),int(opt.imageSize/4+opt.overlapPred):int(opt.imageSize/4+opt.imageSize/2-opt.overlapPred)] = 2*117.0/255.0 - 1.0\n",
    "        input_cropped.data[:,1,int(opt.imageSize/4+opt.overlapPred):int(opt.imageSize/4+opt.imageSize/2-opt.overlapPred),int(opt.imageSize/4+opt.overlapPred):int(opt.imageSize/4+opt.imageSize/2-opt.overlapPred)] = 2*104.0/255.0 - 1.0\n",
    "        input_cropped.data[:,2,int(opt.imageSize/4+opt.overlapPred):int(opt.imageSize/4+opt.imageSize/2-opt.overlapPred),int(opt.imageSize/4+opt.overlapPred):int(opt.imageSize/4+opt.imageSize/2-opt.overlapPred)] = 2*123.0/255.0 - 1.0\n",
    "\n",
    "        # train with real: Discriminator is trained on the real center\n",
    "        netD.zero_grad()\n",
    "        label.data.resize_(batch_size).fill_(real_label)\n",
    "\n",
    "        output = netD(real_center)\n",
    "        errD_real = criterion(output, label)\n",
    "        errD_real.backward()\n",
    "        D_x = output.data.mean()\n",
    "\n",
    "        # train with fake: Generator generates what it thinks is the center\n",
    "        # This fake image is fed to the discriminator\n",
    "        fake = netG(input_cropped)\n",
    "        label.data.fill_(fake_label)\n",
    "        output = netD(fake.detach())\n",
    "        errD_fake = criterion(output, label)\n",
    "        errD_fake.backward()\n",
    "        D_G_z1 = output.data.mean()\n",
    "        errD = errD_real + errD_fake\n",
    "        optimizerD.step()\n",
    "\n",
    "\n",
    "        ############################\n",
    "        # (2) Update G network: maximize log(D(G(z)))\n",
    "        ###########################\n",
    "        netG.zero_grad()\n",
    "        label.data.fill_(real_label)  # fake labels are real for generator cost\n",
    "        output = netD(fake)\n",
    "        errG_D = criterion(output, label)\n",
    "\n",
    "        wtl2Matrix = real_center.clone()\n",
    "        wtl2Matrix.data.fill_(wtl2*overlapL2Weight)\n",
    "        wtl2Matrix.data[:,:,int(opt.overlapPred):int(opt.imageSize/2 - opt.overlapPred),int(opt.overlapPred):int(opt.imageSize/2 - opt.overlapPred)] = wtl2\n",
    "        \n",
    "        errG_l2 = (fake-real_center).pow(2)\n",
    "        errG_l2 = errG_l2 * wtl2Matrix\n",
    "        errG_l2 = errG_l2.mean()\n",
    "\n",
    "        errG = (1-wtl2) * errG_D + wtl2 * errG_l2\n",
    "\n",
    "        errG.backward()\n",
    "\n",
    "        D_G_z2 = output.data.mean()\n",
    "        optimizerG.step()\n",
    "\n",
    "        print('[%d/%d][%d/%d] Loss_D: %.4f Loss_G: %.4f / %.4f l_D(x): %.4f l_D(G(z)): %.4f'\n",
    "              % (epoch, opt.niter, i, len(dataloader),\n",
    "                 errD.data[0], errG_D.data[0],errG_l2.data[0], D_x,D_G_z1, ))\n",
    "        if i % 100 == 0:\n",
    "            vutils.save_image(real_cpu,\n",
    "                    'result/train/real/real_samples_epoch_%03d.png' % (epoch))\n",
    "            vutils.save_image(input_cropped.data,\n",
    "                    'result/train/cropped/cropped_samples_epoch_%03d.png' % (epoch))\n",
    "            recon_image = input_cropped.clone()\n",
    "            recon_image.data[:,:,int(opt.imageSize/4):int(opt.imageSize/4+opt.imageSize/2),int(opt.imageSize/4):int(opt.imageSize/4+opt.imageSize/2)] = fake.data\n",
    "            vutils.save_image(recon_image.data,\n",
    "                    'result/train/recon/recon_center_samples_epoch_%03d.png' % (epoch))\n",
    "\n",
    "\n",
    "    # do checkpointing\n",
    "    torch.save({'epoch':epoch+1,\n",
    "                'state_dict':netG.state_dict()},\n",
    "                'model/netG_streetview.pth' )\n",
    "    torch.save({'epoch':epoch+1,\n",
    "                'state_dict':netD.state_dict()},\n",
    "                'model/netlocalD.pth' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = Options('result/test/cropped/065_im.png', 'netG_streetview.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0820\n"
     ]
    }
   ],
   "source": [
    "# Run test with already loaded weights\n",
    "\n",
    "netG = _netG(opt)\n",
    "netG.load_state_dict(torch.load(opt.netG,map_location=lambda storage, location: storage)['state_dict'])\n",
    "netG.eval()\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "\n",
    "image = utils.load_image(opt.test_image, opt.imageSize)\n",
    "image = transform(image)\n",
    "image = image.repeat(1, 1, 1, 1)\n",
    "\n",
    "input_real = torch.FloatTensor(1, 3, opt.imageSize, opt.imageSize)\n",
    "input_cropped = torch.FloatTensor(1, 3, opt.imageSize, opt.imageSize)\n",
    "real_center = torch.FloatTensor(1, 3, opt.imageSize/2, opt.imageSize/2)\n",
    "\n",
    "criterionMSE = nn.MSELoss()\n",
    "\n",
    "input_real = Variable(input_real)\n",
    "input_cropped = Variable(input_cropped)\n",
    "real_center = Variable(real_center)\n",
    "\n",
    "input_real.data.resize_(image.size()).copy_(image)\n",
    "input_cropped.data.resize_(image.size()).copy_(image)\n",
    "real_center_cpu = image[:,:,opt.imageSize//4:opt.imageSize//4+opt.imageSize//2,opt.imageSize//4:opt.imageSize//4+opt.imageSize//2]\n",
    "real_center.data.resize_(real_center_cpu.size()).copy_(real_center_cpu)\n",
    "\n",
    "input_cropped.data[:,0,opt.imageSize//4+opt.overlapPred:opt.imageSize//4+opt.imageSize//2-opt.overlapPred,opt.imageSize//4+opt.overlapPred:opt.imageSize//4+opt.imageSize//2-opt.overlapPred] = 2*117.0/255.0 - 1.0\n",
    "input_cropped.data[:,1,opt.imageSize//4+opt.overlapPred:opt.imageSize//4+opt.imageSize//2-opt.overlapPred,opt.imageSize//4+opt.overlapPred:opt.imageSize//4+opt.imageSize//2-opt.overlapPred] = 2*104.0/255.0 - 1.0\n",
    "input_cropped.data[:,2,opt.imageSize//4+opt.overlapPred:opt.imageSize//4+opt.imageSize//2-opt.overlapPred,opt.imageSize//4+opt.overlapPred:opt.imageSize//4+opt.imageSize//2-opt.overlapPred] = 2*123.0/255.0 - 1.0\n",
    "\n",
    "# This is where the image with the center missing is fed into the pretrained generator\n",
    "fake = netG(input_cropped)\n",
    "errG = criterionMSE(fake,real_center)\n",
    "\n",
    "recon_image = input_cropped.clone()\n",
    "recon_image.data[:,:,opt.imageSize//4:opt.imageSize//4+opt.imageSize//2,opt.imageSize//4:opt.imageSize//4+opt.imageSize//2] = fake.data\n",
    "\n",
    "utils.save_image('val_real_samples.png',image[0])\n",
    "utils.save_image('val_cropped_samples.png',input_cropped.data[0])\n",
    "utils.save_image('val_recon_samples.png',recon_image.data[0])\n",
    "\n",
    "# print('%.4f' % errG.data[0])\n",
    "print('%.4f' % errG.data.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
